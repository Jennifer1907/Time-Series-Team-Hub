---
title: "Module 6 - Tuáº§n 1 - XÃ¢y dá»±ng Loss Function cho Linear Regression & Logistic Regression tá»« Ä‘áº§u "
date: 2025-11-21T13:03:07+07:00
description: Linear Regression vs Logistic Regression
image: images/linear_logistic.jpg
caption:
categories:
  - minutes
tags:
  - feature
draft: false
---
# ğŸ“˜ XÃ¢y dá»±ng Loss Function cho Linear & Logistic Regression tá»« Ä‘áº§u  
*AIO Vietnam â€“ Module 6, Week 1 â€“ NgÃ y 18/11/2025*

> ğŸ“ BÃ i viáº¿t dá»±a trÃªn ná»™i dung bÃ i giáº£ng:  
> **M6W1D4+6 â€“ Learn to Build Loss Function for Linear Regression & Logistic Regression from the Ground Up**  
> (TÃ i liá»‡u há»c ná»™i bá»™ AIO2025)

---

## ğŸŒŸ Giá»›i thiá»‡u

Loss function lÃ  trÃ¡i tim cá»§a má»i mÃ´ hÃ¬nh Machine Learning.  
BÃ i blog nÃ y giÃºp báº¡n hiá»ƒu sÃ¢u cÃ¡ch **tá»± xÃ¢y dá»±ng Loss Function tá»« con sá»‘ 0**, theo Ä‘Ãºng tinh tháº§n â€œground-upâ€:

- Tá»« trá»±c giÃ¡c â†’ mÃ´ hÃ¬nh tuyáº¿n tÃ­nh  
- Tá»« phÃ¢n phá»‘i Bernoulli â†’ Log-Likelihood  
- Tá»« Log-Likelihood â†’ Binary Cross-Entropy (BCE)  
- Tá»« Ä‘áº¡o hÃ m â†’ Hessian â†’ chá»©ng minh convexity  

Káº¿t quáº£: báº¡n hiá»ƒu báº£n cháº¥t Logistic Regression, chá»© khÃ´ng chá»‰ â€œnhá»› cÃ´ng thá»©câ€.

---

## ğŸ” Ná»™i dung chÃ­nh

### 1ï¸âƒ£ Tá»« Linear Regression â†’ Logistic Regression

Linear Regression phÃ¹ há»£p cho dá»± bÃ¡o liÃªn tá»¥c nhÆ°ng **tháº¥t báº¡i trong phÃ¢n loáº¡i** vÃ¬:

- Output cÃ³ thá»ƒ ngoÃ i khoáº£ng [0, 1], khÃ´ng thá»ƒ xem lÃ  xÃ¡c suáº¥t  
- Khi káº¿t há»£p vá»›i sigmoid + MSE â†’ gradient nhá», mÃ´ hÃ¬nh há»c ráº¥t cháº­m  
- Loss trá»Ÿ thÃ nh **non-convex**, dá»… bá»‹ káº¹t  

Káº¿t luáº­n: cáº§n má»™t Loss Function má»›i phÃ¹ há»£p báº£n cháº¥t **xÃ¡c suáº¥t**.

---

### 2ï¸âƒ£ Loss Function cho Linear Regression

- HÃ m giáº£ thuyáº¿t:  
  `y_hat = Î¸^T x`
- Loss chuáº©n cho há»“i quy:  
  `MSE = 1/(2m) * Î£ (y_hat - y)^2`
- MSE lÃ  **convex**, dá»… tá»‘i Æ°u vÃ  há»™i tá»¥ nhanh  
- NhÆ°ng **khÃ´ng phÃ¹ há»£p** cho Logistic Regression

---

### 3ï¸âƒ£ XÃ¢y dá»±ng Binary Cross-Entropy tá»« phÃ¢n phá»‘i Bernoulli

Vá»›i phÃ¢n loáº¡i nhá»‹ phÃ¢n:

- NhÃ£n tuÃ¢n theo Bernoulli  
- Likelihood:  
  `L = y*log(y_hat) + (1-y)*log(1-y_hat)`
- Äá»•i dáº¥u Ä‘á»ƒ tá»‘i thiá»ƒu hoÃ¡:  
  `BCE = -[ y log(y_hat) + (1-y) log(1-y_hat) ]`

BCE lÃ  Loss Ä‘Æ°á»£c suy ra tá»« **nguyÃªn lÃ½ thá»‘ng kÃª**, khÃ´ng pháº£i ngáº«u há»©ng.

---

### 4ï¸âƒ£ Logistic Regression dáº¡ng vector hoÃ¡

- Dá»¯ liá»‡u dÆ°á»›i dáº¡ng ma tráº­n:  
  `X (mÃ—n), Î¸ (nÃ—1)`
- MÃ´ hÃ¬nh:  
  `y_hat = Ïƒ(XÎ¸)`
- Gradient gá»n:  
  `âˆ‡Î¸ L = X^T (y_hat - y)`

Khi vector hoÃ¡, mÃ´ hÃ¬nh nhanh â€“ gá»n â€“ tá»‘i Æ°u hiá»‡u quáº£.

---

### 5ï¸âƒ£ BCE lÃ  convex â€“ PhÃ¢n tÃ­ch Hessian

- Äáº¡o hÃ m báº­c hai cá»§a BCE:  
  `âˆ‚Â²L/âˆ‚Î¸Â² = xÂ² * y_hat * (1 - y_hat)`
- VÃ¬ cÃ¡c pháº§n tá»­ Ä‘á»u â‰¥ 0 â†’ **Hessian khÃ´ng Ã¢m**

Do Ä‘Ã³:

- BCE **convex**  
- Logistic Regression cÃ³ **global minimum duy nháº¥t**  
- Gradient Descent luÃ´n há»™i tá»¥

---

## ğŸ’¡ VÃ¬ sao bÃ i viáº¿t nÃ y há»¯u Ã­ch?

âœ” Hiá»ƒu â€œtáº­n gá»‘câ€ thay vÃ¬ há»c thuá»™c cÃ´ng thá»©c  
âœ” Biáº¿t vÃ¬ sao MSE khÃ´ng phÃ¹ há»£p cho classification  
âœ” Náº¯m nguyÃªn lÃ½ thá»‘ng kÃª cá»§a Logistic Regression  
âœ” Tháº¥y Ä‘Æ°á»£c vai trÃ² cá»§a convexity trong tá»‘i Æ°u hÃ³a  
âœ” Dá»… dÃ ng tá»± code láº¡i Logistic Regression báº±ng NumPy  

---

## ğŸ–¼ï¸ Gá»£i Ã½ minh hoáº¡ cho blog

Báº¡n cÃ³ thá»ƒ thÃªm 1 trong 3 hÃ¬nh minh hoáº¡ sau:

### ğŸ¨ Concept 1 â€“ MSE vs BCE (Cartoon)
- MSE: máº·t má»‡t má»i, bá»‹ â€œsigmoid Ä‘Ã¨ pháº³ng gradientâ€  
- BCE: siÃªu anh hÃ¹ng cÃ³ Ä‘á»“ thá»‹ convex mÆ°á»£t  
- CÃ¢u thoáº¡i: â€œPhÃ¢n loáº¡i cá»© Ä‘á»ƒ tÃ´i!â€

### ğŸ¨ Concept 2 â€“ Infographic Flow
Timeline 5 bÆ°á»›c:
1. Linear model  
2. Sigmoid  
3. Bernoulli  
4. Likelihood â†’ BCE  
5. Convex optimization  

### ğŸ¨ Concept 3 â€“ Landscape tá»‘i Æ°u
- TrÃ¡i: MSE + sigmoid â†’ Ä‘á»“ thá»‹ gá»“ ghá»  
- Pháº£i: BCE â†’ hÃ¬nh cÃ¡i bÃ¡t mÆ°á»£t  
- Hai nhÃ¢n váº­t nhá» Ä‘ang â€œleo nÃºiâ€ vÃ  â€œtrÆ°á»£t xuá»‘ng Ä‘Ã¡yâ€

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- AIO2025 â€“ Module 6 Week 1  
- Andrew Ng â€“ CS229  
- DeepLearning.AI â€“ Logistic Regression  
- Bishop â€“ Pattern Recognition and Machine Learning  

---

## âœ¨ Káº¿t luáº­n

ÄÃ¢y lÃ  má»™t trong nhá»¯ng bÃ i há»c ná»n táº£ng nhÆ°ng cá»±c ká»³ quan trá»ng trong Machine Learning.  
Hiá»ƒu sÃ¢u Loss Function giÃºp báº¡n:

- TrÃ¡nh mÃ´ hÃ¬nh há»c sai báº£n cháº¥t  
- Tá»‘i Æ°u Ä‘Ãºng hÆ°á»›ng  
- Tá»± tin triá»ƒn khai Logistic Regression thá»±c chiáº¿n  

ChÃºc báº¡n há»c tháº­t vui vÃ  hiá»ƒu tháº­t sÃ¢u! ğŸš€

ğŸ“‚ _TÃ i liá»‡u Ä‘i kÃ¨m:_
{{< pdf src="/Time-Series-Team-Hub/pdf/M6W1D4+6_Learn_to_Build_LossFunction_for_LinearRegression_and_LogisticRegression_from_the_GrounthUp.pdf" title="M6W1D4+6_Learn_to_Build_LossFunction_for_LinearRegression_and_LogisticRegression_from_the_GrounthUp" height="700px" >}}