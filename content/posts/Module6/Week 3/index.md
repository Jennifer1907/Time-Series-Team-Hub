---

title: "Module 6 - Tuáº§n 3: Multilayer Perceptron & Metrics for Classification"
date: 2025-11-28T10:00:00+07:00
description: "Tuáº§n 3 cá»§a Module 6 nÃ¢ng cáº¥p tá»« Softmax Regression lÃªn Multilayer Perceptron (MLP), Ä‘i kÃ¨m kháº£o sÃ¡t Activation, Initialization, Optimizer vÃ  cÃ¡c há»‡ metric dÃ nh cho bÃ i toÃ¡n phÃ¢n loáº¡i. Sinh viÃªn vá»«a há»c cÃ´ng thá»©c forward/backward, vá»«a code PyTorch, Ä‘á»“ng thá»i káº¿t ná»‘i vá»›i tháº¿ giá»›i MLOps qua Prometheus & Grafana."
image: images/MLP_Metrics.png
caption: Illustration by AI Vietnam Team
categories:
  - minutes
tags:

draft: false
---

ğŸ“ **All-in-One Course 2025 â€“ aivietnam.edu.vn**
ğŸ“˜ **Study Guide: Module 6 â€“ Week 3** 
ğŸ§© **Chá»§ Ä‘á»:** Multilayer Perceptron & Metrics for Classification

> ğŸ’¡ *Tuáº§n nÃ y lÃ  bÆ°á»›c chuyá»ƒn tá»« â€œmÃ´ hÃ¬nh tuyáº¿n tÃ­nhâ€ sang â€œmáº¡ng nÆ¡-ron sÃ¢uâ€: MLP, Activation, Initialization, Optimizer â€“ vÃ  cÃ¡ch Ä‘o lÆ°á»ng mÃ´ hÃ¬nh phÃ¢n loáº¡i báº±ng cÃ¡c há»‡ metric khÃ¡c nhau Ä‘á»ƒ chuáº©n bá»‹ cho bÃ i Loss á»Ÿ cÃ¡c tuáº§n sau.*

---

## ğŸ“… **Lá»‹ch trÃ¬nh há»c vÃ  ná»™i dung chÃ­nh**

### ğŸ§‘â€ğŸ« **Thá»© 3 â€“ NgÃ y 18/11/2025**

*(Buá»•i warm-up â€“ MSc. Quá»‘c ThÃ¡i)* 

**Chá»§ Ä‘á»:** MLP cÆ¡ báº£n â€“ tá»« Perceptron tá»›i Multi-layer Perceptron
**Ná»™i dung:**

* Nháº¯c láº¡i Softmax/Logistic Regression, lÃ½ do pháº£i **thÃªm hidden layer**.
* Tháº£o luáº­n cÃ¡c bÆ°á»›c trong **MLP pipeline**: chuáº©n bá»‹ dá»¯ liá»‡u â†’ chuáº©n hÃ³a â†’ xÃ¢y network â†’ khá»Ÿi táº¡o tham sá»‘.
* LÃ m 1 vÃ­ dá»¥ tÃ­nh tay Ä‘Æ¡n giáº£n forward qua 1 hidden layer Ä‘á»ƒ hiá»ƒu:
  $$
  \mathbf{h} = \sigma(W_1 \mathbf{x} + \mathbf{b}_1), \quad
  \hat{\mathbf{y}} = \text{softmax}(W_2 \mathbf{h} + \mathbf{b}_2)
  $$

---

### ğŸ‘¨â€ğŸ« **Thá»© 4 â€“ NgÃ y 19/11/2025**

*(Buá»•i há»c chÃ­nh â€“ Dr. Quang Vinh)* 

**Chá»§ Ä‘á»:** XÃ¢y dá»±ng MLP â€“ Forward & Backward, tá»« lÃ½ thuyáº¿t tá»›i PyTorch
**Ná»™i dung:**

* So sÃ¡nh **Softmax Regression vs MLP**: khi nÃ o tuyáº¿n tÃ­nh lÃ  khÃ´ng Ä‘á»§, vÃ¬ sao cáº§n non-linearity.
* XÃ¢y tá»«ng bÆ°á»›c cÃ´ng thá»©c **forward** vÃ  **backward** cho nhiá»u layer; giáº£i thÃ­ch vÃ¬ sao backprop trÃ´ng â€œrá»‘iâ€ nhÆ°ng thá»±c cháº¥t chá»‰ lÃ  chain rule Ã¡p Ä‘i Ã¡p láº¡i.
* Tháº£o luáº­n cÃ¢u há»i Ä‘au Ä‘áº§u khi design network:

  * Bao nhiÃªu **hidden layers**, má»—i layer bao nhiÃªu **neurons**?
  * Äáº·t **activation** á»Ÿ Ä‘Ã¢u: `fc1 â†’ act â†’ fc2 â†’ act â†’ fc3` hay `fc1 â†’ fc2 â†’ act â†’ fc3` â€“ vÃ  vÃ¬ sao â€œaffine â†’ non-linear â†’ affineâ€ má»›i tháº­t sá»± tÄƒng Ä‘Æ°á»£c nÄƒng lá»±c biá»ƒu diá»…n.
  * Khi nÃ o dÃ¹ng `self.activation = nn.ReLU()` trong `__init__` vs dÃ¹ng `F.relu()` trá»±c tiáº¿p trong `forward`.
* CÃ i Ä‘áº·t MLP báº±ng **PyTorch** cho tabular / image Ä‘Æ¡n giáº£n; luyá»‡n thÃ³i quen Ä‘á»c `model.parameters()` vÃ  xem kÃ­ch thÆ°á»›c tá»«ng layer.

---

### âš™ï¸ **Thá»© 5 â€“ NgÃ y 20/11/2025**

*(Buá»•i MLOps â€“ TA Nguyá»…n Thuáº­n)* 

**Chá»§ Ä‘á»:** Prometheus & Grafana â€“ Tracking & Logging cho há»‡ thá»‘ng AI

**Ná»™i dung:**

* NhÃ¬n láº¡i **MLFlow** tuáº§n trÆ°á»›c, káº¿t ná»‘i sang há»‡ **monitoring/observability** báº±ng Prometheus & Grafana.
* Hiá»ƒu dÃ²ng cháº£y: model MLP huáº¥n luyá»‡n â†’ log metrics, loss, latency â†’ Prometheus thu tháº­p â†’ Grafana váº½ dashboard.
* Demo pipeline nhá»: track má»™t MLP Ä‘ang train (loss, accuracy, thá»i gian batch, GPU memory) vÃ  hiá»ƒn thá»‹ trÃªn Grafana.

---

### ğŸ§  **Thá»© 6 â€“ NgÃ y 21/11/2025**

*(Buá»•i há»c chÃ­nh â€“ Dr. Quang Vinh)* 

**Chá»§ Ä‘á»:** Activation Functions, Initialization & Optimizer trong MLP

**Ná»™i dung:**

* Kháº£o sÃ¡t cÃ¡c **activation function** quan trá»ng:

  * ReLU, LeakyReLU, GELU, Sigmoid, Tanhâ€¦
  * PhÃ¢n tÃ­ch Æ°u/nhÆ°á»£c: ReLU Ä‘Æ¡n giáº£n, nhÆ°ng dá»… bá»‹ **dying ReLU**; GELU mÆ°á»£t hÆ¡n, há»£p vá»›i transformer-style MLP.
* Tháº£o luáº­n chi tiáº¿t:

  * Khi nÃ o nÃªn thay ReLU báº±ng GELU / LeakyReLU?
  * VÃ¬ sao activation Ä‘áº·t **giá»¯a cÃ¡c fully-connected layers** lÃ  máº¥u chá»‘t Ä‘á»ƒ network khÃ´ng sá»¥p thÃ nh má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh duy nháº¥t.
* **Initialization & Optimizer**:

  * He / Xavier initialization, vÃ  liÃªn há»‡ vá»›i loáº¡i activation Ä‘ang dÃ¹ng.
  * Adam, SGD (with momentum): cÃ¡ch chÃºng di chuyá»ƒn trong khÃ´ng gian tham sá»‘, Æ°u â€“ nhÆ°á»£c tá»«ng loáº¡i.
* Thá»±c nghiá»‡m nhanh: thay activation / initializer / optimizer trÃªn cÃ¹ng má»™t MLP, xem áº£nh hÆ°á»Ÿng tá»›i tá»‘c Ä‘á»™ há»™i tá»¥ vÃ  cháº¥t lÆ°á»£ng nghiá»‡m.

---

### ğŸ“Š **Thá»© 7 â€“ NgÃ y 22/11/2025**

*(Buá»•i chuyÃªn Ä‘á» â€“ Dr. ÄÃ¬nh Vinh)*

**Chá»§ Ä‘á»:** Metrics cho BÃ i toÃ¡n PhÃ¢n loáº¡i (Binary, Multiclass, Multilabel)

**Ná»™i dung:**

* Ã”n nhanh khÃ¡i niá»‡m **Confusion Matrix** vÃ  cÃ¡c thÃ nh pháº§n TP, TN, FP, FN. 
* Vá»›i **binary classification**:

  * Accuracy, Precision, Recall (TPR), Specificity (TNR), FPR, FNR, F1.
  * Ká»‹ch báº£n â€œbá»‡nh hiáº¿mâ€ Ä‘á»ƒ tháº¥y vÃ¬ sao **accuracy** cÃ³ thá»ƒ â€œlá»«a mÃ¬nh dá»‘i ngÆ°á»iâ€. 
* Vá»›i **multiclass**:

  * Micro / Macro / Weighted Precisionâ€“Recallâ€“F1, **Balanced Accuracy**, **FÎ²-score** khi muá»‘n Æ°u tiÃªn Recall hÆ¡n Precision. 
* Vá»›i **multilabel**:

  * Exact Match Ratio, 0/1 Loss, **Hamming Loss**, multilabel Precision/Recall/F1. 
* Tháº£o luáº­n: cÃ¡ch chá»n **metric phÃ¹ há»£p vá»›i bÃ i toÃ¡n**, vÃ  vÃ¬ sao nhiá»u metric **khÃ´ng differentiable**, nÃªn ta dÃ¹ng **loss function** nhÆ° Cross-Entropy / Focal Loss lÃ m â€œsurrogateâ€ Ä‘á»ƒ tá»‘i Æ°u.

---

### ğŸ‘¨â€ğŸ“ **Chá»§ nháº­t â€“ NgÃ y 23/11/2025**

*(Buá»•i Ã´n táº­p â€“ TA ÄÃ¬nh Tháº¯ng)* 

**Chá»§ Ä‘á»:** MLP â€“ Exercise & Mini Project

**Ná»™i dung:**

* Ã”n táº­p nhanh ná»™i dung chÃ­nh cá»§a buá»•i thá»© 4 vÃ  thá»© 6: tá»« forward/backward MLP Ä‘áº¿n chá»n activation, init, optimizer.
* Giáº£i bÃ i táº­p code vÃ  giáº¥y:

  * TÃ­nh gradient Ä‘Æ¡n giáº£n cho má»™t MLP 2-layer.
  * Sá»­a má»™t Ä‘oáº¡n PyTorch: chuyá»ƒn tá»« dÃ¹ng `F.relu` sang `self.activation`, thá»­ thay ReLU báº±ng GELU.
* Káº¿t ná»‘i vá»›i buá»•i metric: cháº¡y má»™t mÃ´ hÃ¬nh MLP nhá», log cÃ¡c metric **Accuracy, F1, Recall, Balanced Accuracy**, so sÃ¡nh chÃºng vá»›i nhau.

---

## ğŸ“Œ **Äiá»ƒm nháº¥n vÃ  kiáº¿n thá»©c chÃ­nh**

### âœ… Tá»« Softmax Regression tá»›i Multilayer Perceptron

* Softmax Regression lÃ  **mÃ´ hÃ¬nh tuyáº¿n tÃ­nh** trÃªn feature:\
  $$
  \hat{\mathbf{y}} = \text{softmax}(W\mathbf{x} + \mathbf{b})
  $$
  â†’ khÃ´ng Ä‘á»§ Ä‘á»ƒ há»c cÃ¡c biÃªn quyáº¿t Ä‘á»‹nh phi tuyáº¿n.
* MLP thÃªm **hidden layers + activation** Ä‘á»ƒ biá»ƒu diá»…n hÃ m phi tuyáº¿n phá»©c táº¡p:
  $$
  \mathbf{h}^{(l)} = \sigma(W^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
  $$
* CÃ¡c cÃ¢u há»i thá»±c táº¿ khi design MLP:

  * Bao nhiÃªu layer lÃ  â€œvá»«a Ä‘áº¹pâ€ cho bÃ i toÃ¡n hiá»‡n táº¡i?
  * Layer nÃ o nÃªn má»ng, layer nÃ o nÃªn dÃ y?
  * DÃ¹ng activation gÃ¬, Ä‘áº·t á»Ÿ Ä‘Ã¢u Ä‘á»ƒ trÃ¡nh dying ReLU nhÆ°ng váº«n train nhanh?

---

### âœ… Activation, Initialization & Optimizer â€“ Bá»™ ba quyáº¿t Ä‘á»‹nh â€œtÃ­nh cÃ¡châ€ cá»§a MLP

* **ReLU**: nhanh, Ä‘Æ¡n giáº£n nhÆ°ng cÃ³ nguy cÆ¡ cháº¿t nÆ¡-ron (dying ReLU).
* **LeakyReLU / GELU**: má»m hÆ¡n, giá»¯ gradient tá»‘t hÆ¡n á»Ÿ vÃ¹ng Ã¢m, phÃ¹ há»£p máº¡ng sÃ¢u.
* **Initialization**:

  * He init: há»£p vá»›i ReLU/variants.
  * Xavier: há»£p vá»›i Tanh/Sigmoid.
* **Optimizer**:

  * SGD + Momentum: Ä‘Æ¡n giáº£n, á»•n Ä‘á»‹nh nhÆ°ng cáº§n tuning cáº©n tháº­n.
  * Adam: tá»± Ä‘iá»u chá»‰nh learning rate, ráº¥t Ä‘Æ°á»£c dÃ¹ng cho MLP thá»±c táº¿.

> Vá»›i cÃ¹ng má»™t kiáº¿n trÃºc, chá»‰ cáº§n Ä‘á»•i activation + init + optimizer lÃ  **curve loss/metric** Ä‘Ã£ cÃ³ thá»ƒ khÃ¡c háº³n â€“ Ä‘Ã¢y lÃ  chá»— ráº¥t Ä‘Ã¡ng thá»­ nghiá»‡m trong tuáº§n nÃ y.

---

### âœ… Backprop qua nhiá»u layer â€“ LÃ m chá»§ â€œmáº·t tá»‘iâ€ cá»§a MLP

* Backward MLP thá»±c cháº¥t chá»‰ lÃ  **chain rule** chá»“ng nhiá»u lá»›p:

  * Tá»« (\frac{\partial \mathcal{L}}{\partial \hat{\mathbf{y}}}) â†’ (\frac{\partial \mathcal{L}}{\partial W^{(L)}}), (\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(L-1)}}) â†’ lan ngÆ°á»£c dáº§n xuá»‘ng input.
* Tuáº§n nÃ y táº­p trung giáº£i thÃ­ch **trá»±c giÃ¡c**: má»—i layer há»c cÃ¡ch â€œbiáº¿n Ä‘á»•i khÃ´ng gian featureâ€ sao cho lá»›p cuá»‘i dá»… phÃ¢n tÃ¡ch báº±ng Softmax/Logistic.
* ÄÃ¢y cÅ©ng lÃ  chá»— dá»… dáº«n tá»›i **vanishing/exploding gradient**, nÃªn lá»±a chá»n activation + init trá»Ÿ nÃªn cá»±c ká»³ quan trá»ng.

---

### âœ… Metrics cho Classification & LiÃªn há»‡ vá»›i Loss

* **Binary**: Accuracy, Precision, Recall, Specificity, F1, FPR, FNR.
* **Multiclass**: Micro/Macro/Weighted F1, Balanced Accuracy, FÎ², â€¦ â€“ dÃ¹ng khi dá»¯ liá»‡u **máº¥t cÃ¢n báº±ng** hoáº·c khi muá»‘n coi trá»ng tá»«ng lá»›p khÃ¡c nhau. 
* **Multilabel**: Exact Match Ratio (ráº¥t khÃ³), 0/1 Loss, Hamming Loss, multilabel Precision/Recall/F1. 
* **Káº¿t ná»‘i vá»›i Loss**:

  * Metric lÃ  â€œthÆ°á»›c Ä‘o cháº¥t lÆ°á»£ngâ€ mÃ  ta quan tÃ¢m (F1, Balanced Accuracy, â€¦).
  * Loss lÃ  hÃ m **differentiable** Ä‘á»ƒ backprop (Cross-Entropy, BCEWithLogits, Focal Lossâ€¦).
  * Ta khÃ´ng backprop trá»±c tiáº¿p trÃªn F1, nhÆ°ng sáº½ chá»n **loss phÃ¹ há»£p vá»›i metric má»¥c tiÃªu** (vÃ­ dá»¥: class imbalance â†’ dÃ¹ng Weighted Cross-Entropy / Focal Loss thay vÃ¬ MSE).

---

## ğŸ“š **TÃ i liá»‡u Ä‘i kÃ¨m**

* {{< pdf src="/Time-Series-Team-Hub/pdf/M06W03-StudyGuide.pdf" title="M06W03 â€“ Study Guide" height="700px" >}}
* {{< pdf src="/Time-Series-Team-Hub/pdf/M06W03-MLP.pdf" title="M06W03 â€“ Multilayer Perceptron Slides" height="700px" >}}
* {{< pdf src="/Time-Series-Team-Hub/pdf/M06W03-InsightIntoMLP.pdf" title="M06W03 â€“ Insight into MLP" height="700px" >}}
* {{< pdf src="/Time-Series-Team-Hub/pdf/M06W03-MetricsForClassification.pdf" title="M06W03 â€“ Metrics for Classification" height="700px" >}}

---

ğŸ§  *Repository managed by [AI Vietnam Team Hub](https://github.com/AI-Vietnam-Institution/All-in-One-Course)*
ğŸ“ *Blog thuá»™c series **All-in-One Course 2025** â€“ chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o toÃ n diá»‡n AI, Data Science, vÃ  MLOps táº¡i [aivietnam.edu.vn](https://aivietnam.edu.vn)*
