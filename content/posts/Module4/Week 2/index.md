---
title: "Module 4 - Tuáº§n 2: Tree Series Tiáº¿p Tá»¥c â€“ Gradient Boosting, XGBoost & FastAPI Deployment"
date: 2025-09-14T10:00:00+07:00
description: "Tuáº§n nÃ y chÃºng ta tiáº¿p ná»‘i series Tree-based models vá»›i Gradient Boosting vÃ  XGBoost, há»c tá»« vÃ­ dá»¥ tÃ­nh tay Ä‘áº¿n code tháº­t, kÃ¨m bonus 1 ngÃ y FastAPI Ä‘á»ƒ Ä‘Æ°a mÃ´ hÃ¬nh lÃªn web!"
image: images/Boosting.png
categories:
  - minutes
tags:
  - gradient-boosting
  - xgboost
  - fastapi
  - timeseries
draft: false

---

ğŸ‰ **ChÃ o má»«ng Ä‘áº¿n vá»›i blog Tuáº§n 2 cá»§a team Time Series â€“ Module 4!**

ğŸŒŸ **Giá»›i thiá»‡u**

Sau khi â€œlÃ m báº¡nâ€ vá»›i **Random Forest** vÃ  **AdaBoost** á»Ÿ tuáº§n trÆ°á»›c, tuáº§n nÃ y series Tree-based models tiáº¿p tá»¥c vá»›i hai cao thá»§ Ä‘Ã¬nh Ä‘Ã¡m: **Gradient Boosting** vÃ  **XGBoost**.  

Äiá»ƒm Ä‘áº·c biá»‡t cá»§a tuáº§n nÃ y:  
- **Gradient Boosting**: Ä‘i tá»« vÃ­ dá»¥ tÃ­nh tay tá»«ng bÆ°á»›c Ä‘á»ƒ hiá»ƒu rÃµ cÆ¡ cháº¿ â€œfit residualsâ€, rá»“i code láº¡i vá»›i scikit-learn, kÃ¨m pháº§n **Behind the Scenes** (giáº£i thÃ­ch ná»™i bá»™: residual = negative gradient, leaf value tá»« minimization).  
- **XGBoost**: Ä‘Ã o sÃ¢u **Behind the Scenes** hÆ¡n ná»¯a â€” cÃ´ng thá»©c Taylor báº­c 2, similarity score, regularization â€” vÃ  Ã¡p dá»¥ng cho regression, classification, time series.  
- **FastAPI**: má»™t ngÃ y â€œMLOps nháº¹ nhÃ ngâ€ Ä‘á»ƒ há»c cÃ¡ch biáº¿n model thÃ nh web API vá»›i CRUD, response model, vÃ  mini case study â€œFashion Detection Appâ€.  

---

ğŸ“… **Lá»‹ch trÃ¬nh tuáº§n há»c**

ğŸ—“ï¸ **Thá»© Ba - 09/09/2025**  
ğŸ” **Warm-up: Gradient Boosting cÆ¡ báº£n** *(Extra Class)*  
**Giáº£ng viÃªn:** TA ÄÃ¬nh Tháº¯ng  
- Ã”n láº¡i decision tree, supervised learning.  
- LÃ m **vÃ­ dá»¥ tÃ­nh tay step-by-step** cho regression (MSE) vÃ  classification (logistic loss).  
- Tháº¥y rÃµ tá»«ng bÆ°á»›c: khá»Ÿi táº¡o F0 (mean/log-odds), tÃ­nh pseudo-residuals, chá»n split, tÃ­nh leaf value, update model.  

ğŸ—“ï¸ **Thá»© TÆ° - 10/09/2025**  
ğŸ§  **Gradient Boosting â€“ LÃ½ thuyáº¿t & á»©ng dá»¥ng** *(Main Session)*  
**Giáº£ng viÃªn:** Dr. ÄÃ¬nh Vinh  
- Gradient Boosting cho regression & classification: loss, gradient, leaf output.  
- Pháº§n **Behind the Scenes**: táº¡i sao residual = negative gradient, táº¡i sao dÃ¹ng Taylor approximation Ä‘á»ƒ xáº¥p xá»‰ loss.  
- á»¨ng dá»¥ng vÃ o **time series forecasting**: walk-forward CV, rolling vs expanding window, Ä‘Ã¡nh giÃ¡ báº±ng MAE/MSE/MAPE.  

ğŸ—“ï¸ **Thá»© NÄƒm - 11/09/2025**  
âš¡ **Triá»ƒn khai ML model vá»›i FastAPI** *(Basic MLOps)*  
**Giáº£ng viÃªn:** TA ÄÄƒng NhÃ£  
- Tá»« **khÃ¡i niá»‡m API** (request/response, client-server) â†’ **ASGI & Uvicorn**.  
- Hiá»ƒu async/await Ä‘á»ƒ code concurrent.  
- **CRUD vá»›i FastAPI**: routing, Pydantic models, status codes, `/docs` & `/redoc`.  
- **Case study**: Fashion Detection App â€“ dá»±ng endpoint cho mÃ´ hÃ¬nh object detection, gá»­i áº£nh â†’ tráº£ bounding boxes.  

ğŸ—“ï¸ **Thá»© SÃ¡u - 12/09/2025**  
ğŸš€ **XGBoost (1)** *(Main Session)*  
**Giáº£ng viÃªn:** Dr. ÄÃ¬nh Vinh  
- XGBoost cho regression & classification.  
- **Behind the Scenes**: cÃ´ng thá»©c Taylor báº­c 2 (gradient + Hessian), similarity score Ä‘á»ƒ chá»n split, leaf output.  
- Regularization: Î» (L2), Î³ (min split loss), shrinkage (learning rate).  
- Handling missing values, á»©ng dá»¥ng cho time series.  

ğŸ—“ï¸ **Thá»© Báº£y - 13/09/2025**  
ğŸ“Š **Project Time-series: PG&E Energy Analytics Challenge**  
**Giáº£ng viÃªn:** PhD-c VÃµ NguyÃªn  
- TÃ¬m hiá»ƒu data nÄƒng lÆ°á»£ng, Ä‘áº·c trÆ°ng seasonal/winter-summer.  
- Thá»­ nghiá»‡m Decision Tree, Random Forest, XGBoost.  
- So sÃ¡nh Æ°u/nhÆ°á»£c Ä‘iá»ƒm tá»«ng cÃ¡ch, chá»n chiáº¿n lÆ°á»£c tá»‘t cho dá»± bÃ¡o.  

ğŸ—“ï¸ **Chá»§ Nháº­t - 14/09/2025**  
ğŸ’ª **Exercise Session: Gradient Boosting & XGBoost**  
**Giáº£ng viÃªn:** TA Quá»‘c ThÃ¡i  
- Ã”n táº­p trá»ng tÃ¢m tuáº§n.  
- LÃ m bÃ i táº­p tá»•ng há»£p regression, classification, time series.  
- Cá»§ng cá»‘ hiá»ƒu biáº¿t qua hands-on coding.  

---

ğŸ¯ **Má»¥c tiÃªu há»c táº­p**

ğŸ“Œ **Gradient Boosting**  
- Hiá»ƒu cÆ¡ cháº¿ boosting stage-wise: má»—i cÃ¢y fit pseudo-residuals.  
- LÃ m vÃ­ dá»¥ tay cho regression (MSE) & classification (logistic).  
- Biáº¿t cÃ¡ch kiá»ƒm soÃ¡t overfit báº±ng learning rate, depth nhá», subsample.  
- Hiá»ƒu **Behind the Scenes**: residual = negative gradient, leaf value tá»« minimization/Taylor.  
- Ãp dá»¥ng cho time series vá»›i walk-forward CV.  

ğŸ“Œ **XGBoost**  
- Hiá»ƒu cÃ´ng thá»©c Taylor báº­c 2 (gradient + Hessian).  
- Biáº¿t cÃ¡ch tÃ­nh **Similarity Score**, leaf value.  
- Náº¯m vai trÃ² regularization (Î», Î³).  
- Xá»­ lÃ½ missing values vÃ  á»©ng dá»¥ng cho dá»¯ liá»‡u time series.  

ğŸ“Œ **FastAPI Deployment**  
- Hiá»ƒu khÃ¡i niá»‡m API, ASGI, async/await.  
- Viáº¿t CRUD endpoints, dÃ¹ng Pydantic Ä‘á»ƒ validate input/output.  
- Biáº¿t dá»±ng response model, status code chuáº©n.  
- Triá»ƒn khai mini-app cho mÃ´ hÃ¬nh ML.  

---

ğŸ“‚ **_TÃ i liá»‡u Ä‘i kÃ¨m:_**  
{{< pdf src="/Time-Series-Team-Hub/pdf/M4W2D1+2_GradientBoosting.pdf" title="M4W2D1+2_GradientBoosting" height="700px" >}}  
{{< pdf src="/Time-Series-Team-Hub/pdf/M4W2D3_WebDeploymentUsingFastAPI.pdf" title="M4W2D3_WebDeploymentUsingFastAPI" height="700px" >}}  
{{< pdf src="/Time-Series-Team-Hub/pdf/M4W2D4_XGBoost.pdf" title="M4W2D4_XGBoost" height="700px" >}} 

---

ğŸ§  **_Repository managed by [Time Series Team Hub](https://github.com/Jennifer1907/Time-Series-Team-Hub)_**
