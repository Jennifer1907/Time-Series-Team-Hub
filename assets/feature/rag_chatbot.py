import streamlit as st
import os
import torch
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_huggingface.llms import HuggingFacePipeline
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig
import time
import tempfile
import shutil

st.set_page_config(
    page_title="PDF RAG Assistant",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for ChatGPT-like styling
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 1rem 0;
        margin-bottom: 2rem;
    }

    .chat-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 1rem;
    }

    .user-message {
        background-color: #000000;
        border-radius: 18px;
        padding: 12px 16px;
        margin: 8px 0;
        margin-left: 20%;
        text-align: left;
    }

    .assistant-message {
        background-color: #006400;
        border-radius: 18px;
        padding: 12px 16px;
        margin: 8px 0;
        margin-right: 20%;
        text-align: left;
    }

    .chat-input-container {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background-color: white;
        padding: 1rem;
        border-top: 1px solid #e0e0e0;
        z-index: 1000;
    }

    .stTextInput > div > div > input {
        border-radius: 25px;
        border: 2px solid #e0e0e0;
        padding: 12px 20px;
    }

    .document-info {
        background-color: #f8f9fa;
        border-left: 4px solid #28a745;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 4px;
    }

    .status-indicator {
        display: inline-block;
        width: 12px;
        height: 12px;
        border-radius: 50%;
        margin-right: 8px;
    }

    .status-ready {
        background-color: #28a745;
    }

    .status-loading {
        background-color: #ffc107;
    }

    .status-error {
        background-color: #dc3545;
    }

    .upload-section {
        background-color: #f8f9fa;
        border: 2px dashed #dee2e6;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        text-align: center;
    }

    .method-tab {
        padding: 0.5rem 1rem;
        margin: 0.2rem;
        border-radius: 8px;
        cursor: pointer;
        display: inline-block;
        transition: all 0.3s;
    }

    .method-tab.active {
        background-color: #007bff;
        color: white;
    }

    .method-tab.inactive {
        background-color: #e9ecef;
        color: #6c757d;
    }
</style>
""", unsafe_allow_html=True)

# Session state initialization
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'documents_loaded' not in st.session_state:
    st.session_state.documents_loaded = False
if 'pdf_folder_path' not in st.session_state:
    st.session_state.pdf_folder_path = "./knowledge_base"
if 'upload_method' not in st.session_state:
    st.session_state.upload_method = "folder"
if 'uploaded_files_info' not in st.session_state:
    st.session_state.uploaded_files_info = []

@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")

@st.cache_resource
def load_llm():
    MODEL_NAME = "lmsys/vicuna-7b-v1.5"

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4"
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto"
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    model_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        pad_token_id=tokenizer.eos_token_id,
        device_map="auto"
    )
    return HuggingFacePipeline(pipeline=model_pipeline)

def save_uploaded_files(uploaded_files):
    """Save uploaded files to temporary directory and return paths"""
    temp_dir = tempfile.mkdtemp()
    saved_paths = []
    
    for uploaded_file in uploaded_files:
        # Create temp file path
        temp_path = os.path.join(temp_dir, uploaded_file.name)
        
        # Save uploaded file
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        saved_paths.append(temp_path)
    
    return saved_paths, temp_dir

def load_uploaded_pdfs(uploaded_files):
    """Load PDF files from uploaded files"""
    if not uploaded_files:
        return None, 0, []

    all_documents = []
    loaded_files = []
    
    # Save uploaded files temporarily
    temp_paths, temp_dir = save_uploaded_files(uploaded_files)
    
    progress_bar = st.progress(0)
    status_text = st.empty()

    try:
        for i, (temp_path, uploaded_file) in enumerate(zip(temp_paths, uploaded_files)):
            try:
                status_text.text(f"ƒêang x·ª≠ l√Ω: {uploaded_file.name}")
                
                loader = PyPDFLoader(temp_path)
                documents = loader.load()
                all_documents.extend(documents)
                loaded_files.append(uploaded_file.name)
                progress_bar.progress((i + 1) / len(temp_paths))
                
                st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {uploaded_file.name} ({len(documents)} pages)")
                
            except Exception as e:
                st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω {uploaded_file.name}: {str(e)}")
                continue
    
    finally:
        # Clean up temporary files
        try:
            shutil.rmtree(temp_dir)
        except:
            pass
        
        progress_bar.empty()
        status_text.empty()

    if not all_documents:
        return None, 0, loaded_files

    # Create semantic chunks
    semantic_splitter = SemanticChunker(
        embeddings=st.session_state.embeddings,
        buffer_size=1,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=95,
        min_chunk_size=500,
        add_start_index=True
    )

    docs = semantic_splitter.split_documents(all_documents)
    vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
    retriever = vector_db.as_retriever()

    prompt = hub.pull("rlm/rag-prompt")

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | st.session_state.llm
        | StrOutputParser()
    )
    
    return rag_chain, len(docs), loaded_files

def load_pdfs_from_folder(folder_path):
    """Load all PDF files from the specified folder"""
    cleaned_path = folder_path.strip().strip('"').strip("'")
    folder = Path(cleaned_path)

    st.write(f"üîç **Debug info:**")
    st.write(f"- ƒê∆∞·ªùng d·∫´n g·ªëc: `{folder_path}`")
    st.write(f"- ƒê∆∞·ªùng d·∫´n ƒë√£ l√†m s·∫°ch: `{cleaned_path}`")
    st.write(f"- ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi: `{folder.absolute()}`")
    st.write(f"- Folder t·ªìn t·∫°i: `{folder.exists()}`")

    if not folder.exists():
        st.error(f"‚ùå Folder kh√¥ng t·ªìn t·∫°i: `{cleaned_path}`")
        st.info("üí° **G·ª£i √Ω kh·∫Øc ph·ª•c:**")
        st.info("1. Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n c√≥ ƒë√∫ng kh√¥ng")
        st.info("2. T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥")
        st.info("3. S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n ng·∫Øn h∆°n (v√≠ d·ª•: `C:\\knowledge_base`)")
        return None, 0, []

    pdf_files = list(folder.glob("*.pdf"))
    if not pdf_files:
        st.warning(f"Kh√¥ng th·ªÉ t√¨m th·∫•y file PDF n√†o trong folder: {cleaned_path}")
        return None, 0, []

    all_documents = []
    loaded_files = []

    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, pdf_file in enumerate(pdf_files):
        try:
            status_text.text(f"ƒêang x·ª≠ l√Ω: {pdf_file.name}")
            st.write(f"üîç Processing file: {str(pdf_file)}")

            loader = PyPDFLoader(str(pdf_file))
            documents = loader.load()
            all_documents.extend(documents)
            loaded_files.append(pdf_file.name)
            progress_bar.progress((i + 1) / len(pdf_files))

            st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {pdf_file.name} ({len(documents)} pages)")

        except Exception as e:
            st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω {pdf_file.name}: {str(e)}")
            st.write(f"üîç Chi ti·∫øt l·ªói: {type(e).__name__}")
            continue

    progress_bar.empty()
    status_text.empty()

    if not all_documents:
        return None, 0, loaded_files

    semantic_splitter = SemanticChunker(
        embeddings=st.session_state.embeddings,
        buffer_size=1,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=95,
        min_chunk_size=500,
        add_start_index=True
    )

    docs = semantic_splitter.split_documents(all_documents)
    vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)
    retriever = vector_db.as_retriever()

    prompt = hub.pull("rlm/rag-prompt")

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | st.session_state.llm
        | StrOutputParser()
    )
    return rag_chain, len(docs), loaded_files

def display_chat_message(message, is_user=True):
    """Display a chat message with proper styling"""
    if is_user:
        st.markdown(f"""
        <div class="user-message">
            <strong>B·∫°n:</strong> {message}
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="assistant-message">
            <strong>AI Assistant:</strong> {message}
        </div>
        """, unsafe_allow_html=True)

def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ü§ñ PDF RAG Assistant</h1>
        <p>Tr·ª£ l√Ω AI th√¥ng minh - H·ªèi ƒë√°p v·ªõi t√†i li·ªáu PDF b·∫±ng ti·∫øng Vi·ªát</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar for configuration
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")

        # Model loading status
        if st.session_state.models_loaded:
            st.markdown('<span class="status-indicator status-ready"></span>**Models:** ƒê√£ s·∫µn s√†ng', unsafe_allow_html=True)
        else:
            st.markdown('<span class="status-indicator status-loading"></span>**Models:** ƒêang t·∫£i...', unsafe_allow_html=True)

        # Document loading status
        if st.session_state.documents_loaded:
            st.markdown('<span class="status-indicator status-ready"></span>**T√†i li·ªáu:** ƒê√£ t·∫£i', unsafe_allow_html=True)
        else:
            st.markdown('<span class="status-indicator status-error"></span>**T√†i li·ªáu:** Ch∆∞a t·∫£i', unsafe_allow_html=True)

        st.divider()

        # Method selection
        st.subheader("üìö Ch·ªçn ph∆∞∆°ng th·ª©c t·∫£i t√†i li·ªáu")
        
        # Create tabs for different methods
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üìÅ T·ª´ th∆∞ m·ª•c", use_container_width=True, 
                        type="primary" if st.session_state.upload_method == "folder" else "secondary"):
                st.session_state.upload_method = "folder"
                st.session_state.documents_loaded = False
                st.rerun()
        
        with col2:
            if st.button("üì§ Upload tr·ª±c ti·∫øp", use_container_width=True,
                        type="primary" if st.session_state.upload_method == "upload" else "secondary"):
                st.session_state.upload_method = "upload"
                st.session_state.documents_loaded = False
                st.rerun()

        st.divider()

        # Configuration based on selected method
        if st.session_state.upload_method == "folder":
            st.subheader("üìÅ C·∫•u h√¨nh th∆∞ m·ª•c PDF")

            # Preset folder options
            preset_options = [
                "./knowledge_base",
                "C:/knowledge_base",
                "D:/pdf_docs",
                "T√πy ch·ªânh..."
            ]

            selected_preset = st.selectbox(
                "Ch·ªçn th∆∞ m·ª•c c√≥ s·∫µn:",
                preset_options
            )

            if selected_preset == "T√πy ch·ªânh...":
                folder_path = st.text_input(
                    "ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a PDF:",
                    value=st.session_state.pdf_folder_path,
                    help="Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a c√°c file PDF"
                )
            else:
                folder_path = selected_preset
                st.text_input(
                    "ƒê∆∞·ªùng d·∫´n hi·ªán t·∫°i:",
                    value=folder_path,
                    disabled=True
                )

            if st.button("üîÑ T·∫£i t·ª´ th∆∞ m·ª•c", type="primary", use_container_width=True):
                st.session_state.pdf_folder_path = folder_path
                st.session_state.documents_loaded = False
                st.rerun()

        else:  # upload method
            st.subheader("üì§ Upload file PDF")
            
            uploaded_files = st.file_uploader(
                "Ch·ªçn file PDF ƒë·ªÉ upload:",
                type=['pdf'],
                accept_multiple_files=True,
                help="B·∫°n c√≥ th·ªÉ ch·ªçn nhi·ªÅu file PDF c√πng l√∫c"
            )
            
            if uploaded_files:
                st.write(f"üìã **ƒê√£ ch·ªçn {len(uploaded_files)} file:**")
                for file in uploaded_files:
                    file_size = len(file.getbuffer()) / 1024 / 1024  # MB
                    st.write(f"- {file.name} ({file_size:.1f} MB)")
                
                if st.button("üöÄ X·ª≠ l√Ω file ƒë√£ upload", type="primary", use_container_width=True):
                    st.session_state.uploaded_files_info = [(f.name, len(f.getbuffer())) for f in uploaded_files]
                    st.session_state.documents_loaded = False
                    # Store uploaded files in session state temporarily
                    st.session_state.current_uploaded_files = uploaded_files
                    st.rerun()

        st.divider()

        # Clear chat history
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            st.session_state.chat_history = []
            st.rerun()

        # Reset documents
        if st.button("üîÑ Reset t√†i li·ªáu", use_container_width=True):
            st.session_state.documents_loaded = False
            st.session_state.rag_chain = None
            st.session_state.uploaded_files_info = []
            if hasattr(st.session_state, 'current_uploaded_files'):
                del st.session_state.current_uploaded_files
            st.rerun()

    # Load models first
    if not st.session_state.models_loaded:
        with st.spinner("üöÄ ƒêang kh·ªüi t·∫°o AI models..."):
            st.session_state.embeddings = load_embeddings()
            st.session_state.llm = load_llm()
            st.session_state.models_loaded = True
        st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")
        st.rerun()

    # Load documents based on selected method
    if st.session_state.models_loaded and not st.session_state.documents_loaded:
        if st.session_state.upload_method == "folder":
            with st.spinner("üìö ƒêang t·∫£i t√†i li·ªáu t·ª´ th∆∞ m·ª•c..."):
                rag_chain, num_chunks, loaded_files = load_pdfs_from_folder(st.session_state.pdf_folder_path)
                
                if rag_chain:
                    st.session_state.rag_chain = rag_chain
                    st.session_state.documents_loaded = True
                    
                    # Display document info
                    st.markdown(f"""
                    <div class="document-info">
                        <h4>üìÑ ƒê√£ t·∫£i th√†nh c√¥ng {len(loaded_files)} t√†i li·ªáu PDF t·ª´ th∆∞ m·ª•c:</h4>
                        <ul>
                            {"".join([f"<li>{file}</li>" for file in loaded_files])}
                        </ul>
                        <p><strong>T·ªïng s·ªë chunks:</strong> {num_chunks}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.success("‚úÖ T√†i li·ªáu ƒë√£ s·∫µn s√†ng cho vi·ªác h·ªèi ƒë√°p!")
                else:
                    st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i t√†i li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c.")
        
        elif st.session_state.upload_method == "upload" and hasattr(st.session_state, 'current_uploaded_files'):
            with st.spinner("üìö ƒêang x·ª≠ l√Ω file ƒë√£ upload..."):
                rag_chain, num_chunks, loaded_files = load_uploaded_pdfs(st.session_state.current_uploaded_files)
                
                if rag_chain:
                    st.session_state.rag_chain = rag_chain
                    st.session_state.documents_loaded = True
                    
                    # Display document info
                    st.markdown(f"""
                    <div class="document-info">
                        <h4>üìÑ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {len(loaded_files)} file PDF ƒë√£ upload:</h4>
                        <ul>
                            {"".join([f"<li>{file}</li>" for file in loaded_files])}
                        </ul>
                        <p><strong>T·ªïng s·ªë chunks:</strong> {num_chunks}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    st.success("‚úÖ T√†i li·ªáu ƒë√£ s·∫µn s√†ng cho vi·ªác h·ªèi ƒë√°p!")
                    
                    # Clean up
                    del st.session_state.current_uploaded_files
                else:
                    st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file ƒë√£ upload. Vui l√≤ng th·ª≠ l·∫°i.")

    # Chat interface
    if st.session_state.rag_chain:
        st.markdown("<div class='chat-container'>", unsafe_allow_html=True)

        # Display chat history
        for message in st.session_state.chat_history:
            display_chat_message(message["content"], message["is_user"])

        st.markdown("</div>", unsafe_allow_html=True)

        # Chat input (at the bottom)
        st.markdown("<div style='height: 100px;'></div>", unsafe_allow_html=True)

        # Create two columns for input and button
        col1, col2 = st.columns([4, 1])

        with col1:
            user_question = st.text_input(
                "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...",
                key="user_input",
                placeholder="H·ªèi b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu...",
                on_change=None
            )

        with col2:
            send_button = st.button("üì§ G·ª≠i", type="primary")

        # Process user input
        if (send_button or user_question) and user_question.strip():
            # Add user message to chat history
            st.session_state.chat_history.append({
                "content": user_question,
                "is_user": True
            })

            # Generate response
            with st.spinner("ü§î ƒêang suy nghƒ©..."):
                try:
                    output = st.session_state.rag_chain.invoke(user_question)
                    answer = output.split('Answer:')[1].strip() if 'Answer:' in output else output.strip()

                    # Add assistant response to chat history
                    st.session_state.chat_history.append({
                        "content": answer,
                        "is_user": False
                    })

                except Exception as e:
                    error_message = f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}"
                    st.session_state.chat_history.append({
                        "content": error_message,
                        "is_user": False
                    })

            # Clear input and refresh
            st.rerun()

    else:
        # Welcome message when no documents are loaded
        current_method = "upload file tr·ª±c ti·∫øp" if st.session_state.upload_method == "upload" else "t·∫£i t·ª´ th∆∞ m·ª•c"
        
        st.markdown(f"""
        <div style='text-align: center; padding: 2rem;'>
            <h3>üëã Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi PDF RAG Assistant!</h3>
            <p>Hi·ªán t·∫°i b·∫°n ƒëang s·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c: <strong>{current_method}</strong></p>
            <br>
            <div class="upload-section">
                <h4>üìö ƒê·ªÉ b·∫Øt ƒë·∫ßu, vui l√≤ng ch·ªçn m·ªôt trong hai c√°ch:</h4>
                <br>
                <div style='display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap;'>
                    <div style='text-align: left; max-width: 300px;'>
                        <h5>üìÅ T·ª´ th∆∞ m·ª•c:</h5>
                        <ol>
                            <li>Ch·ªçn ph∆∞∆°ng th·ª©c "T·ª´ th∆∞ m·ª•c"</li>
                            <li>C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c</li>
                            <li>Nh·∫•n "T·∫£i t·ª´ th∆∞ m·ª•c"</li>
                            <li>B·∫Øt ƒë·∫ßu h·ªèi ƒë√°p!</li>
                        </ol>
                    </div>
                    <div style='text-align: left; max-width: 300px;'>
                        <h5>üì§ Upload tr·ª±c ti·∫øp:</h5>
                        <ol>
                            <li>Ch·ªçn ph∆∞∆°ng th·ª©c "Upload tr·ª±c ti·∫øp"</li>
                            <li>Ch·ªçn file PDF t·ª´ m√°y t√≠nh</li>
                            <li>Nh·∫•n "X·ª≠ l√Ω file ƒë√£ upload"</li>
                            <li>B·∫Øt ƒë·∫ßu h·ªèi ƒë√°p!</li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()